# app/risk_detector.py

from app.preprocessing import tokenize_text

HIGH_RISK_PHRASES = [
    "‡∏≠‡∏¢‡∏≤‡∏Å‡∏ï‡∏≤‡∏¢",
    "‡∏Ü‡πà‡∏≤‡∏ï‡∏±‡∏ß‡∏ï‡∏≤‡∏¢",
    "‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏µ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏≠‡∏¢‡∏π‡πà",
    "‡∏à‡∏ö‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï",
    "‡∏ï‡∏≤‡∏¢‡πÑ‡∏õ‡πÄ‡∏•‡∏¢"
]

MEDIUM_RISK_WORDS = [
    "‡∏™‡∏¥‡πâ‡∏ô‡∏´‡∏ß‡∏±‡∏á",
    "‡∏´‡∏°‡∏î‡∏´‡∏ß‡∏±‡∏á",
    "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤",
    "‡πÑ‡∏°‡πà‡πÑ‡∏´‡∏ß‡πÅ‡∏•‡πâ‡∏ß",
    "‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å"
]

def detect_risk_local(text: str):
    text_lower = text.lower()

    # üî¥ ‡∏ï‡∏£‡∏ß‡∏à phrase ‡∏ï‡∏£‡∏á ‡πÜ ‡∏Å‡πà‡∏≠‡∏ô
    for phrase in HIGH_RISK_PHRASES:
        if phrase in text_lower:
            return {
                "risk_level": 3,
                "keywords": [phrase]
            }

    tokens = tokenize_text(text_lower)

    # üü† medium
    found_keywords = []
    for word in MEDIUM_RISK_WORDS:
        if word in tokens:
            found_keywords.append(word)

    if found_keywords:
        return {
            "risk_level": 2,
            "keywords": found_keywords
        }

    return {
        "risk_level": 0,
        "keywords": []
    }
